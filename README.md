# MRP_IPL_Auction_Data_Analysis

Predicting IPL Auction Prices with Machine Learning

Dataâ€‘driven modeling of Indian Premier League (IPL) player auction prices using engineered seasonâ€‘level performance features and supervised regression (Linear Regression, Random Forest, XGBoost). The project also explores feature selection via correlation analysis and outlines paths for future improvement. 

ğŸ¯ Project Goals

Predict a playerâ€™s final auction price from prior-season performance, role, and origin. 

Understand which features matter most via correlation/importance analysis. 

Provide a reproducible pipeline for data prep, modeling, and evaluation. 

ğŸ Context

The IPL auction shapes team composition and value creation; historically, decisions have leaned on subjective assessments. With richer historical data, ML can bring objectivity to pricing and retention strategies. 

ğŸ“¦ Data

Ballâ€‘byâ€‘Ball Match Dataset (Kaggle): ~180k deliveries with batter, bowler, runs, wickets, over/ball, etc. Enables derivation of batting/bowling KPIs. 

Player Auction Dataset (Kaggle): ~900â€“970 records of auction outcomes with player role, style, team, year, origin, and INR amount. 

Years covered: 2013â€“2022 (auction data).
Example stats: mean auction amount â‰ˆ â‚¹21,054,510; max â‚¹162,500,000; 543 unique players. 

ğŸ§ª Features (engineered per season)

Batting: matches, innings, runs, balls faced, not-outs, average, strike rate, 4s/6s, phase splits (PP/Middle/Death).
Bowling: overs, runs conceded, wickets, economy, strike rate, average, phase metrics (PP/Death).
Lowâ€‘participation players (e.g., <4 innings) filtered to reduce noise. 

Label alignment: features from seasons Yâ€‘1 / Yâ€‘2 predict auction price in Y. 

ğŸ§° Methods

Models: Linear Regression, Decision Tree Regression, Random Forest Regression, XGBoost Regression. 

Split: 70% train / 30% test. 

Metrics: RÂ², MAE, RMSE. 

ğŸ” Experiments & Results

Experiment 1: trained on all features â†’ modest performance.

Linear Regression RÂ² â‰ˆ 0.356

Random Forest RÂ² â‰ˆ 0.206

XGBoost RÂ² â‰ˆ 0.156 

Experiment 2: feature selection via correlation (e.g., Batting_Average râ‰ˆ0.844; Average_StrikeRate râ‰ˆ0.844; Four râ‰ˆ0.698, etc.) improved results.

Linear Regression RÂ² â‰ˆ 0.736

Random Forest RÂ² â‰ˆ 0.655

XGBoost RÂ² â‰ˆ 0.654
